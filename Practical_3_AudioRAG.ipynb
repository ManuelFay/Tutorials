{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils ffmpeg\n",
        "!pip install fsspec==2023.9.2\n",
        "!pip install git+https://github.com/illuin-tech/colpali\n",
        "!pip install pdf2image\n",
        "!pip install openai\n",
        "!pip install --no-deps fast-plaid fastkmeans\n",
        "# !pip install torchvision --upgrade\n",
        "!pip install flash-attn==2.7.3 --no-build-isolation\n",
        "!pip install moviepy pydub\n",
        "!pip install --force-reinstall https://github.com/yt-dlp/yt-dlp/archive/master.tar.gz"
      ],
      "metadata": {
        "id": "2Rhb7a9IdyCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files, userdata\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "import io\n",
        "from scipy.io import wavfile\n",
        "import io\n",
        "import base64\n",
        "from scipy.io.wavfile import write\n",
        "import requests\n",
        "from moviepy.editor import VideoFileClip\n",
        "from openai import OpenAI\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "\n",
        "api_key = userdata.get('OPENAI')\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "J3olXcVeLVKS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uploaded = files.upload()  # Choose your .mp4 file\n",
        "# # convert to WAV format\n",
        "# video = VideoFileClip(\"<local_file>.mp4\")\n",
        "# audio = video.audio\n",
        "# audio.write_audiofile(\"audio.wav\")"
      ],
      "metadata": {
        "id": "CmNKVf3aJh8F"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a youtube video from URL\n",
        "!yt-dlp https://www.youtube.com/watch?v=9vM4p9NN0Ts --extract-audio --audio-format wav -o \"audio.%(ext)s\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-_ZWWv-CGDQ",
        "outputId": "f344be0f-3716-4a24-ac98-1745bf9ed928"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=9vM4p9NN0Ts\n",
            "[youtube] 9vM4p9NN0Ts: Downloading webpage\n",
            "[youtube] 9vM4p9NN0Ts: Downloading tv client config\n",
            "[youtube] 9vM4p9NN0Ts: Downloading player 69b31e11-main\n",
            "[youtube] 9vM4p9NN0Ts: Downloading tv player API JSON\n",
            "[youtube] 9vM4p9NN0Ts: Downloading ios player API JSON\n",
            "[youtube] 9vM4p9NN0Ts: Downloading m3u8 information\n",
            "[info] 9vM4p9NN0Ts: Downloading 1 format(s): 251\n",
            "[download] Destination: audio.webm\n",
            "\u001b[K[download] 100% of   75.27MiB in \u001b[1;37m00:00:01\u001b[0m at \u001b[0;32m43.51MiB/s\u001b[0m\n",
            "[ExtractAudio] Destination: audio.wav\n",
            "Deleting original file audio.webm (pass -k to keep)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chunk in 30s WAV files\n",
        "\n",
        "audios = []\n",
        "\n",
        "# Load original audio\n",
        "audio = AudioSegment.from_wav(\"audio.wav\")\n",
        "\n",
        "# Set target frame rate\n",
        "target_rate = 16000\n",
        "chunk_length_ms = 30 * 1000  # 30 seconds\n",
        "\n",
        "# Split and resample each chunk\n",
        "for i in range(0, len(audio), chunk_length_ms):\n",
        "    chunk = audio[i:i + chunk_length_ms]\n",
        "    # Optional: Convert stereo to mono to simplify\n",
        "    chunk = chunk.set_channels(1)\n",
        "\n",
        "    # Resample the chunk\n",
        "    chunk = chunk.set_frame_rate(target_rate)\n",
        "\n",
        "    # Export and convert to numpy array\n",
        "    buf = io.BytesIO()\n",
        "    chunk.export(buf, format=\"wav\")\n",
        "    buf.seek(0)\n",
        "\n",
        "    rate, data = wavfile.read(buf)\n",
        "    audios.append(data)\n",
        "\n",
        "print(f\"Number of chunks: {len(audios)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7UyXtEcK0lA",
        "outputId": "00fa8f73-2711-4c57-8cf9-ec322a50c4fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Audio(audios[23], autoplay=True, rate=16000))"
      ],
      "metadata": {
        "id": "Wkk3tJmvL199"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers.utils.import_utils import is_flash_attn_2_available\n",
        "\n",
        "from colpali_engine.models import ColQwen2_5Omni, ColQwen2_5OmniProcessor\n",
        "\n",
        "\n",
        "model = ColQwen2_5Omni.from_pretrained(\n",
        "    \"vidore/colqwen-omni-v0.1\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda\",  # or \"mps\" if on Apple Silicon\n",
        "    attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
        ").eval()\n",
        "processor = ColQwen2_5OmniProcessor.from_pretrained(\"manu/colqwen-omni-v0.1\")"
      ],
      "metadata": {
        "id": "YCiYUGljcFue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def audio_to_base64(data, rate=16000):\n",
        "    # Example: audios[1] and known sample rate (e.g., 16000)\n",
        "    # Save to BytesIO buffer\n",
        "    buf = io.BytesIO()\n",
        "    write(buf, rate, data)\n",
        "    buf.seek(0)\n",
        "\n",
        "    # Encode to base64\n",
        "    encoded_string = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
        "    return encoded_string\n",
        "\n",
        "def get_results(query: str, k=10):\n",
        "    batch_queries = processor.process_queries([query]).to(model.device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        query_embeddings = model(**batch_queries)\n",
        "\n",
        "    scores = processor.score_multi_vector(query_embeddings, ds)\n",
        "    # get top-5 scores\n",
        "    return scores[0].topk(k).indices.tolist()"
      ],
      "metadata": {
        "id": "OyQm-7CyT00d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Process the inputs by batches of 4\n",
        " # Run inference - docs\n",
        "dataloader = DataLoader(\n",
        "    dataset=audios,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda x: processor.process_audios(x),\n",
        ")\n",
        "\n",
        "ds  = []\n",
        "for batch_doc in tqdm(dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n",
        "        embeddings_doc = model(**batch_doc)\n",
        "    ds.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVy7KSHEOJqs",
        "outputId": "b18017e7-3c9b-4819-fd60-076d6cb60089"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 105/105 [00:53<00:00,  1.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ds[0].shape:\", ds[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lucqxpJyO_tg",
        "outputId": "4da90e53-7c5d-4802-82d0-f416153515e5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ds[0].shape: torch.Size([804, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain LLM scaling laws ?\"\n",
        "res = get_results(query)\n",
        "print(f\"The best audio chunks are: {res}\")\n",
        "# display(Audio(audios[res[0]], autoplay=True, rate=16000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX6Rl7mKOSnz",
        "outputId": "3279afa7-ffaf-4b2a-c11d-0e4f961018b8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best audio chunks are: [102, 96, 35, 97, 108, 24, 92, 128, 84, 83]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content = [\n",
        "              {\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": f\"Answer the query using the audio files. Say which ones were used to answer. Query: {query}\"\n",
        "              }\n",
        "            ]\n",
        "\n",
        "for i in res[:5]:\n",
        "  content += [{\n",
        "                  \"type\": \"text\",\n",
        "                  \"text\": f\"The following is audio chunk # {i}.\"\n",
        "              },\n",
        "              {\n",
        "              \"type\": \"input_audio\",\n",
        "              \"input_audio\": {\n",
        "                  \"data\": audio_to_base64(audios[i]),\n",
        "                  \"format\": \"wav\"\n",
        "              }}]\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-audio-preview\",\n",
        "    modalities=[\"text\", \"audio\"],\n",
        "    audio={\"voice\": \"ballad\", \"format\": \"wav\"},\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": content\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Answer: {completion.choices[0].message.audio.transcript}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpsLCLWzTAnZ",
        "outputId": "381d99df-5da7-4759-9f53-10c317ac3582"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Explain LLM scaling laws ?\n",
            "Answer: LLM scaling laws describe how the performance of large language models improves predictably as you scale up three main factors: model size (in parameters), the size of the dataset (in tokens), and the amount of compute (measured in FLOPs). These laws help guide decisions about how large a model should be given a fixed budget of compute resources.\n",
            "\n",
            "From audio chunks 102 and 108, we learned that for a given amount of compute, you can predict the best size of a model (in terms of parameters) and how it will perform. These scaling laws indicate that scaling up the size of the model, dataset, or compute generally leads to better performance, as long as the scaling is done efficiently.\n",
            "\n",
            "However, as mentioned in chunk 96 and 97, the laws are not perfectly rigid. Small architectural changes or different model designs can slightly shift the scaling curves, but the general trend remains the same: more compute and data lead to better models.\n",
            "\n",
            "In summary, LLM scaling laws are guidelines that researchers use to efficiently build larger language models within a given compute budget. They rely on balancing model size, dataset size, and compute to achieve optimal performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming completion.choices[0].message.audio.data is your base64 audio string\n",
        "wav_bytes = base64.b64decode(completion.choices[0].message.audio.data)\n",
        "\n",
        "with open(\"response.wav\", \"wb\") as f:\n",
        "    f.write(wav_bytes)\n",
        "display(Audio(\"response.wav\", autoplay=True))"
      ],
      "metadata": {
        "id": "v7yZxEjyVLZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUCALhwOR_r4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
